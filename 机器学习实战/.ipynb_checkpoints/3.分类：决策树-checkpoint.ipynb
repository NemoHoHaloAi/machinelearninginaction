{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的主要优势就在于数据形式非常容易理解，而这也是kNN最主要的缺点之一；\n",
    "\n",
    "决策树：\n",
    "\t1. 根据某一特征进行数据集划分\n",
    "\t2. 判断划分后各个子数据集中的数据是否均为同一类型的\n",
    "\t\t是：返回节点类型\n",
    "\t\t否：使用当前子数据集创建新节点，返回步骤1处理新节点\n",
    "\t关键：\n",
    "\t\t1. 何时停止划分数据集，也是递归结束的条件ID3（ID3处理如何划分数据集，以及何时停止）；\n",
    "\t\t2. 如何决定使用那个特征划分数据集，根据香农熵，选择最大信息增益的特征划分；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集的大原则是:将**无序**的数据变得更加**有序**，方法有很多也各有优劣，其中一种方法是使用信息论度量信息；\n",
    "\n",
    "划分数据集前后信息的变化称之为信息增益，知道如何计算信息增益，我们就可以获取到使得信息增益最大的那个特征作为划分数据集的特征，此处需要使用香农熵来计算信息增益；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算香农熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义，如果待分类的事务可能划分在多个分类之中，则符号 x_i 的信息定义为：\n",
    "\tl(x_i) = -log_2 p(x_i)\n",
    "\t其中p(x_i)是选择该分类的概率；\n",
    "\t为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：\n",
    "\t-sum(p(x_i)*log_2 p(x_i)) i={1,2,3.....n} n为目标变量总类别数\n",
    "\t从公式可以看出含义是：各个类别自身的概率乘以自己的信息量，最后求和得到熵；\n",
    "    \n",
    "    可以说香农熵就是用于度量数据分类的无序程度，因此分类越多，熵会越大；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcShannonEntropy(dataSet):\n",
    "\t\t\"\"\"\n",
    "\t\t统计类别信息\n",
    "\t\t1. 获取到所有分类，创建字典\n",
    "\t\t2. 为每个字典设置其对应分类在数据集中数量\n",
    "\t\t3. 通过数量和总数计算熵\n",
    "\t\t\"\"\"\n",
    "\t\tclasses = {}\n",
    "\t\tfor d in dataSet:\n",
    "\t\t\tclasses[d[-1]] = classes.get(d[-1],0)+1 # 如果没有则设置-1+1，即0，如果有则加1\n",
    "\t\tcount = 1.*len(dataSet)\n",
    "\t\tentropy = -sum([classes[k]/count*math.log(classes[k]/count,2) for k in classes.keys()])\n",
    "\t\treturn entropy\n",
    "\n",
    "calcShannonEntropy(np.array([[1,1,'A'],[1,2,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1219280948873624"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcShannonEntropy(np.array([[1,1,'A'],[1,2,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B'],[3,2,'A'],[3,2,'F'],[3,2,'E'],[3,2,'E'],[3,2,'D']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "熵越高,则混合的数据也越多；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet, axis, value): # 改善一下，原代码是根据value，每次返回当前数据集中对应特征值为value的数据子集，改为返回所有value的数据子集，以字典形式返回\n",
    "\t\"\"\"\n",
    "\t将数据集根据某个特征axis分割为每个特征值对应的数据子集，以dict形式返回\n",
    "\t\"\"\"\n",
    "\tdataDict = {}\n",
    "\tfor d in dataSet:\n",
    "\t\tv = dataDict.get(d[axis],[]) # 从列表中创建集合是Python语言得到列表中唯一元素值的最快方法\n",
    "\t\tv.append(list(np.append(d[:axis],d[axis+1:])))\n",
    "\t\tdataDict[d[axis]] = v\n",
    "\treturn dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': [['1', 'A'], ['2', 'B']],\n",
       " '2': [['1', 'A'], ['3', 'A']],\n",
       " '3': [['2', 'B']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(np.array([[1,1,'A'],[1,2,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]), 0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择最佳特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeature2Split(dataSet):\n",
    "\t\"\"\"\n",
    "\t选择将数据集划分后总熵最小的那个特征以及对应的熵值，数据子集的dict形式返回\n",
    "\t\"\"\"\n",
    "\taxiss = len(dataSet[0])-1 # 去掉最后一个目标变量，否则肯定是该变量最符合要求，按照目标变量划分数据再用目标变量来评估xD\n",
    "\tminEntropy = 99999999 # 最初的无序度量值,用于与划分完之后的数据集计算的熵值进行比较\n",
    "\tminAxis = -1\n",
    "\tminDataDict = None\n",
    "\tfor axis in range(axiss):\n",
    "\t\tdataDict = splitDataSet(dataSet, axis, None)\n",
    "\t\tsumEntropy = sum([calcShannonEntropy(dataDict[k]) for k in dataDict.keys()])\n",
    "\t\tif sumEntropy < minEntropy:\n",
    "\t\t\tminEntropy = sumEntropy\n",
    "\t\t\tminAxis = axis\n",
    "\t\t\tminDataDict = dataDict\n",
    "\tprint 'min entropy:'+str(minEntropy)\n",
    "\tprint 'min axis:'+str(minAxis)\n",
    "\treturn minEntropy, minAxis, minDataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min entropy:0.0\n",
      "min axis:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " 1,\n",
       " {'1': [['1', 'A'], ['2', 'A']],\n",
       "  '2': [['1', 'B'], ['3', 'B']],\n",
       "  '3': [['2', 'A']]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chooseBestFeature2Split(np.array([[1,1,'A'],[1,2,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 递归构建决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递归结束的条件是:程序**遍历完**所有划分数据集的属性,或者每个分支下的所有实例都具有\n",
    "**相同**的分类。如果所有实例具有相同的分类,则得到一个叶子节点或者终止块。任何到达叶子节\n",
    "点的数据必然属于叶子节点的分类。\n",
    "\n",
    "如果数据集已经处理了所有属性,但是类标签依然不是唯一\n",
    "的,此时我们需要决定如何定义该叶子节点,在这种情况下,我们通常会采用**多数表决**的方法决\n",
    "定该叶子节点的分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def majorityCnt(classList):\n",
    "\t\"\"\"\n",
    "\t计算最多的分类并返回，用于处理在所有特征都使用完之后数据子集中依然存在多个分类的case\n",
    "\t\"\"\"\n",
    "\tclassDict = {}\n",
    "\tfor c in classList:\n",
    "\t\tclassDict[c] = classDict.get(c,0)+1\n",
    "\treturn sorted(classDict.items(),key = lambda x:x[1],reverse = True)[0][0]\n",
    "\n",
    "majorityCnt(['a','b','b','a','a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min entropy:0.918295834054\n",
      "min axis:0\n",
      "数据集在同一分类：B\n",
      "数据集在同一分类：B\n",
      "min entropy:1.0\n",
      "min axis:0\n",
      "投票表决：A\n",
      "数据集在同一分类：A\n"
     ]
    }
   ],
   "source": [
    "def createTree(dataSet, labels):\n",
    "    \"\"\"\n",
    "    递归构建决策树\n",
    "    结束条件：\n",
    "        1. 当前数据集均为同一分类，此时熵为0；\n",
    "        2. 当前数据集已经没有特征可以用于划分；\n",
    "    返回：\n",
    "        1. 返回分类标签\n",
    "    \"\"\"\n",
    "    if calcShannonEntropy(dataSet) == 0:\n",
    "        print '数据集在同一分类：'+str(labels[0])\n",
    "        return labels[0]\n",
    "    elif len(dataSet[0])==1:\n",
    "        print '投票表决：'+str(majorityCnt(labels))\n",
    "        return majorityCnt(labels)\n",
    "    else:\n",
    "        _,_,dataDict = chooseBestFeature2Split(dataSet)\n",
    "        for k in dataDict.keys():\n",
    "            createTree(dataDict[k], [i[-1] for i in dataDict[k]])\n",
    "\n",
    "createTree(np.array([[1,1,'B'],[2,1,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]), ['A','B','A','A','B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min entropy:0.918295834054\n",
      "min axis:0\n",
      "min entropy:1.0\n",
      "min axis:0\n",
      "{'feature 1': {'1': 'DataSet in same class:B', '3': 'DataSet in same class:B', '2': {'feature 2': {'1': 'Majority count:A', '3': 'DataSet in same class:A'}}}}\n"
     ]
    }
   ],
   "source": [
    "def createTree(dataSet, labels):\n",
    "\t\"\"\"\n",
    "\t递归构建决策树\n",
    "\t结束条件：\n",
    "\t\t1. 当前数据集均为同一分类，此时熵为0；\n",
    "\t\t2. 当前数据集已经没有特征可以用于划分；\n",
    "\t返回：\n",
    "\t\t1. 返回分类标签\n",
    "\t\"\"\"\n",
    "\tif calcShannonEntropy(dataSet) == 0:\n",
    "\t\treturn 'DataSet in same class:'+str(dataSet[0][-1])\n",
    "\t\t#return dataSet[0][-1]\n",
    "\telif len(dataSet[0])==1:\n",
    "\t\treturn 'Majority count:'+str(majorityCnt(list(np.array(dataSet)[:,-1])))\n",
    "\t\t#return majorityCnt(list(np.array(dataSet)[:,-1]))\n",
    "\t\n",
    "\t_,bestFeat,dataDict = chooseBestFeature2Split(dataSet)\n",
    "\tbestFeatLabel = labels[bestFeat]\n",
    "\tdel labels[bestFeat]\n",
    "\tmyTree = {bestFeatLabel:{}}\n",
    "\tfor k in dataDict.keys():\n",
    "\t\ttmpLabels = labels[:]\n",
    "\t\tmyTree[bestFeatLabel][k] = createTree(dataDict[k], tmpLabels)#[i[-1] for i in dataDict[k]])\n",
    "\treturn myTree\n",
    "\t\t\t\n",
    "myTree = createTree(np.array([[1,1,'B'],[2,1,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]), ['feature 1','feature 2','target'])\n",
    "print myTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用matplotlib注解绘制树形图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试、存储分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试算法：使用决策树解决分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, labels, testVec):\n",
    "    '''\n",
    "    使用决策树进行递归分类任务\n",
    "    tree - 使用的树本身\n",
    "    labels - 树的labels\n",
    "    testVec - 测试的数据点\n",
    "    '''\n",
    "    firstLabel = tree.keys()[0]\n",
    "    nextTree = tree[firstLabel]\n",
    "    firstLabelIndex = labels.index(firstLabel)\n",
    "    for k in nextTree.keys():\n",
    "        if k == testVec[firstLabelIndex]:\n",
    "            if type(nextTree[k]).__name__ == 'dict':\n",
    "                return classify(nextTree[k], labels, testVec)\n",
    "            else:\n",
    "                return nextTree[k] # 说明到达了叶子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSet in same class:B\n"
     ]
    }
   ],
   "source": [
    "print classify(myTree, ['feature 1', 'feature 2', 'target'], ['1', '2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority count:A\n"
     ]
    }
   ],
   "source": [
    "print classify(myTree, ['feature 1', 'feature 2', 'target'], ['2', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSet in same class:A\n"
     ]
    }
   ],
   "source": [
    "print classify(myTree, ['feature 1', 'feature 2', 'target'], ['2', '3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 存储分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeTree(tree, filename):\n",
    "    import pickle\n",
    "    with open(filename, 'w') as fw:\n",
    "        pickle.dump(tree, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabTree(filename):\n",
    "    import pickle\n",
    "    with open(filename, 'r') as fr:\n",
    "        return pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeTree(myTree, 'myTree.tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.无监督学习：利用 K-均值聚类算法对未标注 数据分组.ipynb\r\n",
      "11.无监督学习：使用 Apriori 算法进行关联 分析.ipynb\r\n",
      "12.无监督学习：使用 FP-growth 算法来高效 发现频繁项集.ipynb\r\n",
      "13.其他工具：利用 PCA 来简化数据.ipynb\r\n",
      "14.其他工具：利用 SVD 简化数据.ipynb\r\n",
      "15.其他工具：大数据与 MapReduce.ipynb\r\n",
      "1.机器学习基础.ipynb\r\n",
      "2.分类：KNN.ipynb\r\n",
      "3.分类：决策树.ipynb\r\n",
      "4.分类：基于概率论的分类方法：朴素贝叶斯.ipynb\r\n",
      "5.分类：Logistic回归.ipynb\r\n",
      "6.分类：SVM.ipynb\r\n",
      "7.分类：利用Adaboost元算法提高分类性能.ipynb\r\n",
      "8.利用回归预测数值型数据：预测数值型数据-回归.ipynb\r\n",
      "9.利用回归预测数值型数据：树回归.ipynb\r\n",
      "myTree.tree\r\n",
      "readme.md\r\n",
      "机器学习实战-中文版.pdf\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature 1': {'1': 'DataSet in same class:B', '3': 'DataSet in same class:B', '2': {'feature 2': {'1': 'Majority count:A', '3': 'DataSet in same class:A'}}}}\n"
     ]
    }
   ],
   "source": [
    "print grabTree('myTree.tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例：使用决策树预测隐形眼镜类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('../datas/lenses.txt', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['young', 'myope', 'no', 'reduced', 'no lenses']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = [line.replace('\\n', '').split('\\t') for line in data.readlines()]\n",
    "labels = ['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "data_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min entropy:1.55458516934\n",
      "min axis:3\n",
      "min entropy:1.5683182557\n",
      "min axis:2\n",
      "min entropy:0.918295834054\n",
      "min axis:1\n",
      "min entropy:0.0\n",
      "min axis:0\n",
      "min entropy:0.918295834054\n",
      "min axis:1\n",
      "min entropy:0.0\n",
      "min axis:0\n"
     ]
    }
   ],
   "source": [
    "tree = createTree(data_list, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['age', 'prescript', 'astigmatic', 'tearRate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSet in same class:no lenses\n",
      "DataSet in same class:no lenses\n",
      "DataSet in same class:hard\n"
     ]
    }
   ],
   "source": [
    "print classify(tree, labels, ['young', 'myope', 'no', 'reduced'])\n",
    "print classify(tree, labels, ['young', 'hyper', 'yes', 'reduced'])\n",
    "print classify(tree, labels, ['presbyopic', 'myope', 'yes', 'normal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树分类器就像带有终止块的流程图,终止块表示分类结果。开始处理数据集时,我们首\n",
    "先需要测量集合中数据的不一致性,也就是熵,然后寻找最优方案划分数据集,直到数据集中的\n",
    "所有数据属于同一分类。ID3算法可以用于划分标称型数据集。构建决策树时,我们通常采用递\n",
    "归的方法将数据集转化为决策树。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
