{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost\n",
    "\n",
    "所谓元算法，就是对其他算法的一种有机组合，Adaboost就是一种有名的boosting算法；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "dataPath = '../../../git_mlaction/machinelearninginaction/Ch07/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于数据集多重抽样的分类器\n",
    "\n",
    "ensemble method，或者叫meta algorithm，即算法的算法；\n",
    "\n",
    "1. 不同分类器的组合；\n",
    "2. 同一分类器不同设置的组合；\n",
    "3. 不同数据集应用与同一/不同分类器实例的组合；\n",
    "\n",
    "即集成方法中的各个分类器，可以是算法不同、参数不同、训练数据集不同；\n",
    "\n",
    "优点：\n",
    "* 泛化错误率低；\n",
    "* 易编码实现；\n",
    "* 应用于大部分分类器上，无参数调整（参数在分类器本身上已经设置好了）；\n",
    "\n",
    "缺点：\n",
    "* 对离群点敏感；\n",
    "\n",
    "适用数据类型：数值型和标称型；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bagging：基于数据随机重抽样的分类器构建方法\n",
    "\n",
    "自举汇聚法（bootstrap aggregating），即bagging法，思路是从原始数据集合中有放回的重抽样S个等大小的数据集（即每个数据集中的数据都是从原始数据中有放回的抽取来的，因此可能出现某些元素重复，某些元素不存在的情况），将这S个数据集应用于某个分类算法，得到S个分类器，预测数据时，通过这S个分类器来投票表决，一些更先进的bagging算法，比如随机森林（RF）等；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boosting：基于上一个分类器构建当前分类器\n",
    "\n",
    "思路与bagging不同的是，boosting的每个分类器，是串行的，即每个分类器依赖于上一个分类器的分类结果，并集中关注上一个分类器分错的数据点，这一特性导致boosting的每个分类器的结果权重不是一致的，而bagging中每个分类器的权重都是一样的，典型有Adaboost；\n",
    "\n",
    "理解：可以理解为三个臭皮匠（弱分类器），比如目前有三个难题，我们的方式并不是每个臭皮匠解决一个难题（并行），而是先由臭皮匠1解决三个难题ABC，例如A对了BC错了，那么臭皮匠2就主要关注BC，A就降低关注度（样本权重更新），假如AB对了C错了，那么臭皮匠3主要关注C，次要是B，最后是A，假设ABC都对了，那么对于最终的结果，我们会更相信3号，到2号，最后是1号（分类器权重）；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于错误提升分类器的性能\n",
    "\n",
    "样本权重更新：初始都是一样的，每个分类器分类完毕后，对于分类正确的样本，降低权重，对于分类错误的样本，提高权重；\n",
    "\n",
    "分类器权重更新：可以理解为分类错误率越高，权重越低；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于单层决策树构建弱分类器\n",
    "\n",
    "所谓单层决策树，就是一个树桩，只有一次分裂过程；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测试数据\n",
    "def loadSimpData():\n",
    "    dataMatrix = matrix([\n",
    "        [1., 2.1],\n",
    "        [2., 1.1],\n",
    "        [1.3, 1.],\n",
    "        [1., 1.],\n",
    "        [2., 1.]\n",
    "    ])\n",
    "    classLabels = [1., 1., -1., -1., 1.]\n",
    "    return dataMatrix, mat(classLabels)\n",
    "\n",
    "dataMatrix, classLabels = loadSimpData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f96a3800f50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96c8732690>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_1 = dataMatrix[array(classLabels == 1)[0]]\n",
    "data_0 = dataMatrix[array(classLabels == -1)[0]]\n",
    "plt.scatter(array(data_1[:,0].T)[0], array(data_1[:,1].T)[0])\n",
    "plt.scatter(array(data_0[:,0].T)[0], array(data_0[:,1].T)[0], marker='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断传入的matrix的dimen列的所有值是否小于/大于threshVal，如果是，则设置为-1\n",
    "def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):\n",
    "    retArray = ones((shape(dataMatrix)[0], 1))\n",
    "    if threshIneq == 'lt': # 小于，gt是大于\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.\n",
    "    return retArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建单层决策树，通过在每个特征上设置分隔直线来对数据进行划分，并计算错误值，找到错误值最低的分隔直线\n",
    "def buildStump(dataMat, classLabels, D):\n",
    "    '''\n",
    "    D:各样本的初始权重值向量\n",
    "    '''\n",
    "    dataMatrix, labelMatrix = mat(dataMat), mat(classLabels).T\n",
    "    m, n = shape(dataMatrix)\n",
    "    numSteps = 10. # 控制内循环的步长，即循环几步\n",
    "    bestStump = {}\n",
    "    bestClassEst = mat(zeros((m, 1)))\n",
    "    minError = inf # 设置当前错误值为无穷大\n",
    "    for i in range(n):# 外循环遍历每一个特征\n",
    "        rangeMin, rangeMax = dataMatrix[:,i].min(), dataMatrix[:,i].max()\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps\n",
    "        for j in range(-1, int(numSteps)+1): # 循环每个步长，即-1到10,共11个值\n",
    "            for inequal in ['lt', 'gt']: # 遍历每个不等号\n",
    "                threshVal = (rangeMin + float(j) * stepSize)\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)\n",
    "                errArr = mat(ones((m, 1)))\n",
    "                errArr[predictedVals == labelMatrix] = 0\n",
    "                weightedError = D.T * errArr\n",
    "                #print 'Split: dim: %d, thresh: %.2f, thresh ineqal: %s, weighted error: %.3f' % (i, threshVal, inequal, weightedError)\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClassEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClassEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'ineq': 'lt', 'thresh': 1.3}, matrix([[ 0.2]]), array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numSample = shape(dataMatrix)[0]\n",
    "buildStump(dataMatrix, classLabels, mat(ones((numSample,1))/numSample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上，其实就是简单的通过一个固定的步数（numStep）来将各个特征的坐标轴通过直线划开（比如轴上数值范围是2~8，那么最终循环情况就是从1.4到8，保证覆盖到所有数据，每个点代表一根直线，再加上lt和gt表现直线的左边是-1还是1，然后通过划分开后的结果与实际结果对比，计算错误值，循环，最终得到一个错误值最小的case，此case对应三个关键数据：1. 特征、2. 数据点、3. 大于还是小于）；\n",
    "\n",
    "问题：很直观的感受是，numStep是随机指定的，这个值应该跟数据量有关，或者说跟对应轴上所有数据点的数值个数有关，否则容易出现数据点太多，而numStep比较小，导致分隔线不够密集；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整Adaboost算法的实现\n",
    "\n",
    "之前只是单个弱分类器的实现，而Adaboost的核心是通过串行的方式，训练多个弱分类器，一步一步提升错误率，直到满足某个条件后退出（此处我们的条件是错误率为0）；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost实现\n",
    "def adaboostTrainDS(dataMat, classLabels, numIt = 40): # 控制以下最大迭代数为40，避免无限训练下去\n",
    "    weakClassArr = [] # 存放多个弱分类器的list\n",
    "    m = shape(dataMat)[0]\n",
    "    D = mat(ones((m, 1))/m) # D作为每个分类器都要使用并更新的，体现了每个分类器以来上一个分类器结果的特点\n",
    "    aggClassEst = mat(zeros((m, 1)))\n",
    "    for i in range(numIt):\n",
    "        #print 'D:', D.T\n",
    "        stump, error, classEst = buildStump(dataMat, classLabels, D) # 基于上一步的D\n",
    "        alpha = float(.5 * log((1.0 - error)/max(error, 1e-16))) # 计算分类器自身权重，基于错误率\n",
    "        stump['alpha'] = alpha\n",
    "        weakClassArr.append(stump)\n",
    "        #print 'ClassEst:', classEst.T\n",
    "        # **更新D**，可以看出更新D使用了本轮输出类别结果和实际结果以及本轮权重\n",
    "        expon = multiply(-1 * alpha * mat(classLabels).T, classEst)\n",
    "        D = multiply(D, exp(expon))\n",
    "        D = D / D.sum()\n",
    "        aggClassEst += alpha * classEst # 意思是aggClassEst是当前所有分类器的输出相加的总结果，也就是adaboost的结果\n",
    "        #print 'AggClassEst:', aggClassEst\n",
    "        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T, ones((m, 1))) # sign函数取出数据的符号，比如负的，全转为-1，正的全转为+1\n",
    "        errorRate = aggErrors.sum() / m\n",
    "        #print 'Total error:', errorRate, '\\n'\n",
    "        if errorRate == 0: # 错误率为0，跳出循环\n",
    "            print 'Total iter:', i+1\n",
    "            break\n",
    "        \n",
    "    return weakClassArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iter: 3\n"
     ]
    }
   ],
   "source": [
    "weakClassArr = adaboostTrainDS(dataMatrix, classLabels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'alpha': 0.6931471805599453, 'dim': 0, 'ineq': 'lt', 'thresh': 1.3},\n",
       " {'alpha': 0.9729550745276565, 'dim': 1, 'ineq': 'lt', 'thresh': 1.0},\n",
       " {'alpha': 0.8958797346140273,\n",
       "  'dim': 0,\n",
       "  'ineq': 'lt',\n",
       "  'thresh': 0.90000000000000002}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weakClassArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上，Adaboost算法主流程是一个循环（串行），首先获取基于当前D的最佳DS，生成对应的alpha，计算新的D，计算错误率看看是否满足退出条件；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试算法：基于Adaboost的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaboost的分类函数\n",
    "def adaClassify(data, weakClassArr):\n",
    "    dataMatrix = mat(data)\n",
    "    m = shape(dataMatrix)[0]\n",
    "    aggClassEst = mat(zeros((m, 1)))\n",
    "    for i in range(len(weakClassArr)):\n",
    "        weakClassify = weakClassArr[i]\n",
    "        classEst = stumpClassify(dataMatrix, weakClassify['dim'], weakClassify['thresh'], weakClassify['ineq'])\n",
    "        aggClassEst += weakClassify['alpha'] * classEst\n",
    "        #print 'No.'+str(i+1)+':\\n'+str(aggClassEst)\n",
    "    return sign(aggClassEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result:\n",
      "[[-1.]]\n",
      "Final result:\n",
      "[[-1.]\n",
      " [-1.]]\n",
      "Final result:\n",
      "[[-1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "print 'Final result:\\n'+str(adaClassify([0, 0], weakClassArr))\n",
    "print 'Final result:\\n'+str(adaClassify([[0, 1], [1, 0]], weakClassArr))\n",
    "print 'Final result:\\n'+str(adaClassify([[1, 1], [2, 3]], weakClassArr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例：在一个难数据集上应用Adaboost\n",
    "\n",
    "之前在LR项目用过的马疝病数据集，当时的平均错误率有30%+，主要是因为数据集本身存在很大程度的缺失导致；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(fn):\n",
    "    dataArr, labelArr = [], []\n",
    "    with open(fn) as f:\n",
    "        for line in f.readlines():\n",
    "            features = line.strip().split()\n",
    "            dataArr.append([float(features[i])for i in range(len(features)-1)])\n",
    "            labelArr.append(float(features[-1]))\n",
    "    return dataArr, labelArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumIter: 1, TestSet ErrorRate: 0.268656716418\n",
      "NumIter: 2, TestSet ErrorRate: 0.268656716418\n",
      "NumIter: 3, TestSet ErrorRate: 0.238805970149\n",
      "NumIter: 4, TestSet ErrorRate: 0.238805970149\n",
      "NumIter: 5, TestSet ErrorRate: 0.253731343284\n",
      "NumIter: 6, TestSet ErrorRate: 0.268656716418\n",
      "NumIter: 7, TestSet ErrorRate: 0.253731343284\n",
      "NumIter: 8, TestSet ErrorRate: 0.238805970149\n",
      "NumIter: 9, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 10, TestSet ErrorRate: 0.238805970149\n",
      "NumIter: 11, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 12, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 13, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 14, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 15, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 16, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 17, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 18, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 19, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 20, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 21, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 22, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 23, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 24, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 25, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 26, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 27, TestSet ErrorRate: 0.194029850746\n",
      "NumIter: 28, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 29, TestSet ErrorRate: 0.194029850746\n",
      "NumIter: 30, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 31, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 32, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 33, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 34, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 35, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 36, TestSet ErrorRate: 0.223880597015\n",
      "NumIter: 37, TestSet ErrorRate: 0.194029850746\n",
      "NumIter: 38, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 39, TestSet ErrorRate: 0.194029850746\n",
      "NumIter: 40, TestSet ErrorRate: 0.194029850746\n",
      "NumIter: 41, TestSet ErrorRate: 0.194029850746\n",
      "NumIter: 42, TestSet ErrorRate: 0.194029850746\n",
      "NumIter: 43, TestSet ErrorRate: 0.194029850746\n",
      "NumIter: 44, TestSet ErrorRate: 0.194029850746\n",
      "NumIter: 45, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 46, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 47, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 48, TestSet ErrorRate: 0.208955223881\n",
      "NumIter: 49, TestSet ErrorRate: 0.208955223881\n"
     ]
    }
   ],
   "source": [
    "dataArrTrain, labelArrTrain = loadDataSet(dataPath+'horseColicTraining2.txt')\n",
    "dataArrTest, labelArrTest = loadDataSet(dataPath+'horseColicTest2.txt')\n",
    "for numIter in range(1,50):\n",
    "    weakClassArr = adaboostTrainDS(dataArrTrain, labelArrTrain, numIter)\n",
    "    predictArr = adaClassify(dataArrTest, weakClassArr)\n",
    "    errArr = mat(ones((len(labelArrTest), 1)))\n",
    "    errArr[predictArr == mat(labelArrTest).T] = 0\n",
    "    errorRate = errArr.sum() / len(labelArrTest)\n",
    "    print 'NumIter: '+str(numIter)+', TestSet ErrorRate: '+str(errorRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上，可以看到在达到0.19后，错误率没有继续下降，说明模型在当前数据集上达到了最优，因此使用一个0.19当中，分类器个数最少的就好，这样符合奥卡姆剃刀法则，这里我们选择27，实际场景下这种情况无法通过模型优化，应该在特征工程上下功夫；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非均衡问题\n",
    "\n",
    "所谓非均衡问题，指的是类别的预测错误产生的后果是不尽相同的，比如对于邮件，预测为垃圾邮件且错误的话，意味着我们可能会错失一封重要的邮件而影响工作、生活等，而预测为正常邮件且错误的话，无非就是邮箱中出现一封垃圾邮件而已，很明显，这二者的代价是严重不一致的，即我们更希望系统犯后面的错误而不是前面的，这就是非均衡问题，实际场景下，很多问题都是非均衡的；\n",
    "\n",
    "因此对于预测系统的指标评估，我们不能简单的使用对错来用于评估非均衡问题，需要其他指标；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他分类性能度量指标：正确率、召回率、ROC曲线\n",
    "\n",
    "之前我们使用的指标都是错误率，这个指标有一个严重的问题是，它掩盖了错误的类型，通过上述我们直到错误通常是有两类的，通过**混淆矩阵**，我们可以直观的感受一下具体的错误类型：\n",
    "\n",
    "        马是否患有疝病问题的混淆矩阵：\n",
    "        预测结果    是                             否\n",
    "        实际结果\n",
    "        是         正确(TP)                       错误（FN将有病的马预测为无病）\n",
    "        否         错误（FP将无病的马预测为有病）     正确(TN)\n",
    "        \n",
    "可以看到，该问题中，错误是有两类的，这取决与类别的个数；\n",
    "\n",
    "根据二类混淆矩阵，我们可以定义很多其他的指标来帮助我们处理非均衡问题（其实就是指标的关注点问题）：\n",
    "* 正确率：TP/(TP+FP)，即预测为“是”的case中实际也为“是”占的比例，该值高，表示预测为是的case中大部分都对了，适用于马疝病、垃圾邮件；\n",
    "* 召回率：TP/(TP+FN)，即实际为“是”的case中被预测为“是”占的比例，该值高，表示实际为是的case大部分都被预测到了，适用于癌症检测；\n",
    "* ROC曲线：它表示的是当阈值变化时，FP和TP的关系，理想的情况是高TP，低FP，也就是点出现在左上角，对于ROC的一个评价是线下面积的大小，最佳状态是1.0，随机猜测是0.5；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于代价函数的分类器决策控制\n",
    "\n",
    "所谓代价函数即对于混淆矩阵中的各种组合，每一种对应的惩罚/奖励值是不一样的，这样就可以在一定程度上对分类器的决策起到控制作用，并左右分类器的算法实现，比如Adaboost的D的更新、NB中选择最小期望代价而不是最大概率、SVM中更新参数C等；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理非均衡问题的数据抽样方法\n",
    "\n",
    "对于非均衡问题，可以通过欠抽样、过抽样来进行数据改造，相应的会产生各自的问题，举例训练数据中有100例正常的，1000例不正常的：\n",
    "* 欠抽样：即将1000例不正常的中离决策边界最远的900例直接删除，带来的问题是可能损失有价值的特征信息；\n",
    "* 过抽样：对100例正常的进行复制、插值等处理，补充900例假的模拟数据，带来的问题是可能过拟合（反复利用同样的数据）；\n",
    "\n",
    "目前更常见的做法是二者结合；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "集成方法通过组合多个分类器（通常是弱分类器，避免强分类器本身存在一定过拟合，组合后过拟合更加严重的问题），达到更好的分类效果，方式有两种，bagging方法通过随机抽样得到不同的数据集，训练不同的分类器，boosting更进一步的是每个分类器的训练都是针对上一个分类器的结果，有针对性的进行的（好像我们学生时代考试啊，查漏补缺），因此通常将boosting会得到更好的结果，常用弱分类器有单层决策树；\n",
    "\n",
    "非均衡问题的体现一个是数据集的不均衡，一个是混淆矩阵中各个类型对应后果的不均衡，基于混淆矩阵，我们可以通过正确率、召回率、ROC曲线等重新定义指标来更好的评估性能，而通过欠抽样和过抽样，可以在数据改造方面对非均衡问题进行一定的处理，当然也存在相应的后果要考虑；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
