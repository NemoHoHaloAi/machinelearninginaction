{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于概率论的分类方法：朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于贝叶斯决策理论的分类方法\n",
    "朴素贝叶斯\n",
    "\n",
    "* 优点:在数据较少的情况下仍然有效,可以处理多类别问题。\n",
    "* 缺点:对于输入数据的准备方式较为敏感。\n",
    "* 适用数据类型:标称型数据。\n",
    "\n",
    "假设有两个概率分布的数据类别A，B，我们已知二者的统计参数，pA(x,y)表示数据点x,y属于类别A的概率，pB(x,y)表示数据点属于类别B的概率，那么当pA>pB时，我们认为该数据点为A类，反之为B类，这就是朴素贝叶斯分类的基础思想；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：假设有7块石头，3块是灰色的，4块是黑色的，那么取到灰色石头的概率就是3/7，黑色的则为4/7，这很简单，但是如果7块石头是分别放在两个桶中的呢，其中A桶有2块灰色、2块黑色，B桶有1块灰色、2块黑色，那么此时取到灰色石头概率怎么计算呢？\n",
    "\n",
    "    计算方法1：\n",
    "    p(灰色|桶A) = p(灰色 and 桶A)/p(桶A) = (2/7)/(4/7) = 1/2\n",
    "    p(灰色|桶B) = p(灰色 and 桶B)/p(桶B) = (1/7)/(3/7) = 1/3\n",
    "    计算方法2-贝叶斯方法：\n",
    "    已知p(x|y)求p(y|x)可以通过以下公式：\n",
    "    p(x|y)=((p(y|x)*p(x))/(p(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用条件概率来分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常指的是以下的方式：\n",
    "    \n",
    "    p(c1|x,y,z...) > p(c2|x,y,z...) 时，判断为类别1\n",
    "    p(c1|x,y,z...) < p(c2|x,y,z...) 时，判断为类别2\n",
    "    \n",
    "    而p(c|x,y,z...)可以由贝叶斯公式由其他三个概率求得p(x,y,z...|c)*p(c)/p(x,y,z...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用朴素贝叶斯进行文档分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档分类最简单的方式就是将每个单词的出现与否作为一个特征，这样我们就会得到一个跟词汇表中单词个数一样多的特征数量，这个是非常庞大的，而我们知道根据维度灾难，训练所需数据是呈指数上升的，假设有1000个单词特征，也就需要N^1000的数据量，这个是非常庞大的一个数值，但是如果我们假设特征独立，那么就只需要1000\\*N就可以了，这个至少还在可控范围内，这个假设在文档分类中的表现就是它假设每个单词的出现与其他单词没有关系，当然我们知道这个假设是不太合理的，比如I通常会跟着am一起出现，这也就是朴素的由来，另一个朴素贝叶斯的假设是每个特征的权重一致，当然这一点严格讲也不太合理，但是即便有它不合理的地方，效果还是有的；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用python进行分类\n",
    "\n",
    "朴素贝叶斯分类器通常有两种实现方式：\n",
    "* 贝努利模型实现；\n",
    "* 多项式模型实现；\n",
    "\n",
    "区别在于贝努利模型不考虑每个词的出现次数，只考虑是否出现，因此可以说该方式假设每个词都是等权重的，而多项式则是会考虑出现次数的；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据：从文本转换成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练数据（测试数据）\n",
    "def loadDataSet():\n",
    "    postingList = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'],\n",
    "    ]\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建词汇表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([]) # 空set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # 利用set的不重复属性来获取数据集中所有持续过的单词\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子转换到向量 - 词集模型\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) # 全为0的向量，用numpy的zeros也行\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1 # 贝努利模型只关注是否出现，而不关注次数\n",
    "        else:\n",
    "            print 'The word: %s is not in our vocab list!' % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n"
     ]
    }
   ],
   "source": [
    "listPosts, listClasses = loadDataSet()\n",
    "listVocab = createVocabList(listPosts)\n",
    "print listVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print setOfWords2Vec(listVocab, listPosts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print setOfWords2Vec(listVocab, listPosts[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练算法：从词向量计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.04166667,  0.04166667,  0.04166667,  0.        ,  0.        ,\n",
       "         0.04166667,  0.04166667,  0.04166667,  0.        ,  0.04166667,\n",
       "         0.04166667,  0.04166667,  0.04166667,  0.        ,  0.        ,\n",
       "         0.08333333,  0.        ,  0.        ,  0.04166667,  0.        ,\n",
       "         0.04166667,  0.04166667,  0.        ,  0.04166667,  0.04166667,\n",
       "         0.04166667,  0.        ,  0.04166667,  0.        ,  0.04166667,\n",
       "         0.04166667,  0.125     ]),\n",
       " array([ 0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "         0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "         0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "         0.05263158,  0.05263158,  0.05263158,  0.        ,  0.10526316,\n",
       "         0.        ,  0.05263158,  0.05263158,  0.        ,  0.10526316,\n",
       "         0.        ,  0.15789474,  0.        ,  0.05263158,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB训练函数，目的是得到各个类别对应的条件概率\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix) # 训练集文档个数\n",
    "    numWords = len(trainMatrix[0]) # 词汇表长度\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs) # 类别为侮辱性的比例\n",
    "    p0Num, p1Num = zeros(numWords), zeros(numWords) # 创建空向量\n",
    "    p0Denom, p1Denom = 0.0, 0.0 # \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i] # 类别为1的所有文档的词汇表向量和\n",
    "            p1Denom += sum(trainMatrix[i]) # 类别为1的所有文档的单词个数和\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num/p1Denom # 类别为1的每个出现的单词占所有文档词汇量的比例\n",
    "    p0Vect = p0Num/p0Denom\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "trainMatrix = [setOfWords2Vec(listVocab, document)for document in listPosts]\n",
    "trainNB0(trainMatrix, listClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述函数结果可以看到，用于解决实际问题还存在几个点需求修复：\n",
    "* 如果某个词没有出现过，那么会出现概率为0，因为最终结果是各个词的概率乘积，因此最终结果也为0，解决该方法可以将每个词的出现次数初始化为1，而总数分母初始化为2；\n",
    "* 下溢出，由于结果是很多个概率相乘得到，而python中多个很小的小数相乘结果会四舍五入为0，为了避免这一错误，可以采用求自然对数的方式来处理，一个是自然对数的计算不会有损失，另一个是自然对数虽然结果与原来不同，但是对最终的比较没有影响；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,\n",
       "        -2.56494936, -2.56494936, -2.56494936, -3.25809654, -2.56494936,\n",
       "        -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,\n",
       "        -2.15948425, -3.25809654, -3.25809654, -2.56494936, -3.25809654,\n",
       "        -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,\n",
       "        -2.56494936, -3.25809654, -2.56494936, -3.25809654, -2.56494936,\n",
       "        -2.56494936, -1.87180218]),\n",
       " array([-3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,\n",
       "        -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,\n",
       "        -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,\n",
       "        -2.35137526, -2.35137526, -2.35137526, -3.04452244, -1.94591015,\n",
       "        -3.04452244, -2.35137526, -2.35137526, -3.04452244, -1.94591015,\n",
       "        -3.04452244, -1.65822808, -3.04452244, -2.35137526, -3.04452244,\n",
       "        -3.04452244, -3.04452244]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 解决某个词未出现导致全部乘积结果为0以及下溢出问题\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    # 修改单词默认出现次数为1，总数为2\n",
    "    #p0Num, p1Num = zeros(numWords), zeros(numWords)\n",
    "    #p0Denom, p1Denom = 0.0, 0.0\n",
    "    p0Num, p1Num = ones(numWords), ones(numWords)\n",
    "    p0Denom, p1Denom = 2.0, 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 修改条件概率的计算为取自然对数，避免下溢出问题\n",
    "    #p1Vect = p1Num/p1Denom\n",
    "    #p0Vect = p0Num/p0Denom\n",
    "    p1Vect = log(p1Num/p1Denom)\n",
    "    p0Vect = log(p0Num/p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "trainMatrix = [setOfWords2Vec(listVocab, document)for document in listPosts]\n",
    "trainNB0(trainMatrix, listClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试算法\n",
    "\n",
    "上面已经根据实际情况（1. 下溢出问题，2. 单词一次都不出现导致最终的乘积结果恒为0的问题）修改了计算条件概率的函数，下面我们要测试一下实际效果；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类函数\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    '''\n",
    "    vec2Classify:待测试的文档向量\n",
    "    p0Vec:类别0的条件概率向量\n",
    "    p1Vec:类别1的条件概率向量\n",
    "    pClass1:类别为1的概率\n",
    "    '''\n",
    "    p1 = sum(vec2Classify*p1Vec) + log(pClass1) # ???\n",
    "    p0 = sum(vec2Classify*p0Vec) + log(1-pClass1)\n",
    "    print p0,p1,p1-p0\n",
    "    return 1 if p1>p0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "def testingNB():\n",
    "    listPosts, listClasses = loadDataSet()\n",
    "    listVocab = createVocabList(listPosts)\n",
    "    trainMatrix = [setOfWords2Vec(listVocab, document)for document in listPosts]\n",
    "    p0Vec, p1Vec, pClass1 = trainNB0(trainMatrix, listClasses)\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    print testEntry, 'classify as:', classifyNB(setOfWords2Vec(listVocab, testEntry), p0Vec, p1Vec, pClass1)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    print testEntry, 'classify as:', classifyNB(setOfWords2Vec(listVocab, testEntry), p0Vec, p1Vec, pClass1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classify as: 0\n",
      "['stupid', 'garbage'] classify as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据：文档词袋模型\n",
    "\n",
    "前面的setOfWords2Vec方式使用的文档词集模型，即贝努利模型，只关注词是否持续，而不关注出现次数，而我们知道出现次数也是一个重要特征，因此我们这里使用词袋模型代替词集模型；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子转换到向量 - 词集模型\n",
    "def bagOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1 # 词袋模型关注次数\n",
    "        else:\n",
    "            print 'The word: %s is not in our vocab list!' % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例：使用NB过滤垃圾邮件\n",
    "\n",
    "NB最著名的应用领域：邮件过滤；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据：切分文本\n",
    "\n",
    "使用正则表达式进行切割，针对特定领域、场景有特别的表达式，此处仅使用简单、通用的表达式即可，切割效果会有些许瑕疵；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "regEx = re.compile('\\\\W*')\n",
    "def document2WordList(doc):\n",
    "    listOfTokens = regEx.split(doc)\n",
    "    return [token.lower() for token in listOfTokens if len(token)>0]\n",
    "print document2WordList(mySent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试算法：使用NB进行交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.txt\t13.txt\t16.txt\t19.txt\t21.txt\t24.txt\t3.txt  6.txt  9.txt\r\n",
      "11.txt\t14.txt\t17.txt\t1.txt\t22.txt\t25.txt\t4.txt  7.txt\r\n",
      "12.txt\t15.txt\t18.txt\t20.txt\t23.txt\t2.txt\t5.txt  8.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../datas/email/ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['codeine', '15mg', '30', 'for', '203', '70', 'visa', 'only', 'codeine', 'methylmorphine', 'is', 'a', 'narcotic', 'opioid', 'pain', 'reliever', 'we', 'have', '15mg', '30mg', 'pills', '30', '15mg', 'for', '203', '70', '60', '15mg', 'for', '385', '80', '90', '15mg', 'for', '562', '50', 'visa', 'only'], ['hi', 'peter', 'with', 'jose', 'out', 'of', 'town', 'do', 'you', 'want', 'to', 'meet', 'once', 'in', 'a', 'while', 'to', 'keep', 'things', 'going', 'and', 'do', 'some', 'interesting', 'stuff', 'let', 'me', 'know', 'eugene'], ['hydrocodone', 'vicodin', 'es', 'brand', 'watson', 'vicodin', 'es', '7', '5', '750', 'mg', '30', '195', '120', '570', 'brand', 'watson', '7', '5', '750', 'mg', '30', '195', '120', '570', 'brand', 'watson', '10', '325', 'mg', '30', '199', '120', '588', 'noprescription', 'required', 'free', 'express', 'fedex', '3', '5', 'days', 'delivery', 'for', 'over', '200', 'order', 'major', 'credit', 'cards', 'e', 'check'], ['yay', 'to', 'you', 'both', 'doing', 'fine', 'i', 'm', 'working', 'on', 'an', 'mba', 'in', 'design', 'strategy', 'at', 'cca', 'top', 'art', 'school', 'it', 's', 'a', 'new', 'program', 'focusing', 'on', 'more', 'of', 'a', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'to', 'management', 'i', 'm', 'an', '1', '8', 'of', 'the', 'way', 'done', 'today'], ['you', 'have', 'everything', 'to', 'gain', 'incredib1e', 'gains', 'in', 'length', 'of', '3', '4', 'inches', 'to', 'yourpenis', 'permanantly', 'amazing', 'increase', 'in', 'thickness', 'of', 'yourpenis', 'up', 'to', '30', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe', 'the', 'proven', 'naturalpenisenhancement', 'that', 'works', '100', 'moneyback', 'guaranteeed'], ['what', 'is', 'going', 'on', 'there', 'i', 'talked', 'to', 'john', 'on', 'email', 'we', 'talked', 'about', 'some', 'computer', 'stuff', 'that', 's', 'it', 'i', 'went', 'bike', 'riding', 'in', 'the', 'rain', 'it', 'was', 'not', 'that', 'cold', 'we', 'went', 'to', 'the', 'museum', 'in', 'sf', 'yesterday', 'it', 'was', '3', 'to', 'get', 'in', 'and', 'they', 'had', 'free', 'food', 'at', 'the', 'same', 'time', 'was', 'a', 'sf', 'giants', 'game', 'when', 'we', 'got', 'done', 'we', 'had', 'to', 'take', 'the', 'train', 'with', 'all', 'the', 'giants', 'fans', 'they', 'are', '1', '2', 'drunk'], ['percocet', '10', '625', 'mg', 'withoutprescription', '30', 'tabs', '225', 'percocet', 'a', 'narcotic', 'analgesic', 'is', 'used', 'to', 'treat', 'moderate', 'to', 'moderately', 'severepain', 'top', 'quality', 'express', 'shipping', '100', 'safe', 'discreet', 'private', 'buy', 'cheap', 'percocet', 'online'], ['yo', 'i', 've', 'been', 'working', 'on', 'my', 'running', 'website', 'i', 'm', 'using', 'jquery', 'and', 'the', 'jqplot', 'plugin', 'i', 'm', 'not', 'too', 'far', 'away', 'from', 'having', 'a', 'prototype', 'to', 'launch', 'you', 'used', 'jqplot', 'right', 'if', 'not', 'i', 'think', 'you', 'would', 'like', 'it'], ['codeine', '15mg', '30', 'for', '203', '70', 'visa', 'only', 'codeine', 'methylmorphine', 'is', 'a', 'narcotic', 'opioid', 'pain', 'reliever', 'we', 'have', '15mg', '30mg', 'pills', '30', '15mg', 'for', '203', '70', '60', '15mg', 'for', '385', '80', '90', '15mg', 'for', '562', '50', 'visa', 'only'], ['there', 'was', 'a', 'guy', 'at', 'the', 'gas', 'station', 'who', 'told', 'me', 'that', 'if', 'i', 'knew', 'mandarin', 'and', 'python', 'i', 'could', 'get', 'a', 'job', 'with', 'the', 'fbi'], ['oem', 'adobe', 'microsoft', 'softwares', 'fast', 'order', 'and', 'download', 'microsoft', 'office', 'professional', 'plus', '2007', '2010', '129', 'microsoft', 'windows', '7', 'ultimate', '119', 'adobe', 'photoshop', 'cs5', 'extended', 'adobe', 'acrobat', '9', 'pro', 'extended', 'windows', 'xp', 'professional', 'thousand', 'more', 'titles'], ['hello', 'since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'google', 'groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'or', 'files', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'february', '2011', 'we', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'google', 'groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'instead', 'of', 'these', 'features', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'as', 'google', 'docs', 'and', 'google', 'sites', 'for', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'google', 'sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', 'with', 'the', 'members', 'of', 'your', 'group', 'you', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', 'on', 'the', 'site', 'if', 'you', 're', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'we', 'suggest', 'you', 'try', 'google', 'docs', 'you', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'a', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', 'or', 'an', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'google', 'groups'], ['bargains', 'here', 'buy', 'phentermin', '37', '5', 'mg', 'k', '25', 'buy', 'genuine', 'phentermin', 'at', 'low', 'cost', 'visa', 'accepted', '30', '130', '50', '60', '219', '00', '90', '292', '50', '120', '366', '00', '180', '513', '00'], ['zach', 'hamm', 'commented', 'on', 'your', 'status', 'zach', 'wrote', 'doggy', 'style', 'enough', 'said', 'thank', 'you', 'good', 'night'], ['you', 'have', 'everything', 'to', 'gain', 'incredib1e', 'gains', 'in', 'length', 'of', '3', '4', 'inches', 'to', 'yourpenis', 'permanantly', 'amazing', 'increase', 'in', 'thickness', 'of', 'yourpenis', 'up', 'to', '30', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe'], ['this', 'e', 'mail', 'was', 'sent', 'from', 'a', 'notification', 'only', 'address', 'that', 'cannot', 'accept', 'incoming', 'e', 'mail', 'please', 'do', 'not', 'reply', 'to', 'this', 'message', 'thank', 'you', 'for', 'your', 'online', 'reservation', 'the', 'store', 'you', 'selected', 'has', 'located', 'the', 'item', 'you', 'requested', 'and', 'has', 'placed', 'it', 'on', 'hold', 'in', 'your', 'name', 'please', 'note', 'that', 'all', 'items', 'are', 'held', 'for', '1', 'day', 'please', 'note', 'store', 'prices', 'may', 'differ', 'from', 'those', 'online', 'if', 'you', 'have', 'questions', 'or', 'need', 'assistance', 'with', 'your', 'reservation', 'please', 'contact', 'the', 'store', 'at', 'the', 'phone', 'number', 'listed', 'below', 'you', 'can', 'also', 'access', 'store', 'information', 'such', 'as', 'store', 'hours', 'and', 'location', 'on', 'the', 'web', 'at', 'http', 'www', 'borders', 'com', 'online', 'store', 'storedetailview_98'], ['bargains', 'here', 'buy', 'phentermin', '37', '5', 'mg', 'k', '25', 'buy', 'genuine', 'phentermin', 'at', 'low', 'cost', 'visa', 'accepted', '30', '130', '50', '60', '219', '00', '90', '292', '50', '120', '366', '00', '180', '513', '00'], ['hi', 'peter', 'these', 'are', 'the', 'only', 'good', 'scenic', 'ones', 'and', 'it', 's', 'too', 'bad', 'there', 'was', 'a', 'girl', 's', 'back', 'in', 'one', 'of', 'them', 'just', 'try', 'to', 'enjoy', 'the', 'blue', 'sky', 'd'], ['ordercializviagra', 'online', 'save', '75', '90', '0nline', 'pharmacy', 'noprescription', 'required', 'buy', 'canadian', 'drugs', 'at', 'wholesale', 'prices', 'and', 'save', '75', '90', 'fda', 'approved', 'drugs', 'superb', 'quality', 'drugs', 'only', 'accept', 'all', 'major', 'credit', 'cards'], ['ryan', 'whybrew', 'commented', 'on', 'your', 'status', 'ryan', 'wrote', 'turd', 'ferguson', 'or', 'butt', 'horn'], ['you', 'have', 'everything', 'to', 'gain', 'incredib1e', 'gains', 'in', 'length', 'of', '3', '4', 'inches', 'to', 'yourpenis', 'permanantly', 'amazing', 'increase', 'in', 'thickness', 'of', 'yourpenis', 'up', 'to', '30', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe', 'the', 'proven', 'naturalpenisenhancement', 'that', 'works', '100', 'moneyback', 'guaranteeed'], ['arvind', 'thirumalai', 'commented', 'on', 'your', 'status', 'arvind', 'wrote', 'you', 'know', 'reply', 'to', 'this', 'email', 'to', 'comment', 'on', 'this', 'status'], ['buy', 'ambiem', 'zolpidem', '5mg', '10mg', '2', '39', 'pill', '30', 'pills', 'x', '5', 'mg', '129', '00', '60', 'pills', 'x', '5', 'mg', '199', '20', '180', 'pills', 'x', '5', 'mg', '430', '20', '30', 'pills', 'x', '10', 'mg', '138', '00', '120', 'pills', 'x', '10', 'mg', '322', '80'], ['thanks', 'peter', 'i', 'll', 'definitely', 'check', 'in', 'on', 'this', 'how', 'is', 'your', 'book', 'going', 'i', 'heard', 'chapter', '1', 'came', 'in', 'and', 'it', 'was', 'in', 'good', 'shape', 'i', 'hope', 'you', 'are', 'doing', 'well', 'cheers', 'troy'], ['ordercializviagra', 'online', 'save', '75', '90', '0nline', 'pharmacy', 'noprescription', 'required', 'buy', 'canadian', 'drugs', 'at', 'wholesale', 'prices', 'and', 'save', '75', '90', 'fda', 'approved', 'drugs', 'superb', 'quality', 'drugs', 'only', 'accept', 'all', 'major', 'credit', 'cards', 'order', 'today', 'from', '1', '38'], ['jay', 'stepp', 'commented', 'on', 'your', 'status', 'jay', 'wrote', 'to', 'the', 'reply', 'to', 'this', 'email', 'to', 'comment', 'on', 'this', 'status', 'to', 'see', 'the', 'comment', 'thread', 'follow', 'the', 'link', 'below'], ['buyviagra', '25mg', '50mg', '100mg', 'brandviagra', 'femaleviagra', 'from', '1', '15', 'per', 'pill', 'viagranoprescription', 'needed', 'from', 'certified', 'canadian', 'pharmacy', 'buy', 'here', 'we', 'accept', 'visa', 'amex', 'e', 'check', 'worldwide', 'delivery'], ['linkedin', 'kerry', 'haloney', 'requested', 'to', 'add', 'you', 'as', 'a', 'connection', 'on', 'linkedin', 'peter', 'i', 'd', 'like', 'to', 'add', 'you', 'to', 'my', 'professional', 'network', 'on', 'linkedin', 'kerry', 'haloney'], ['you', 'have', 'everything', 'to', 'gain', 'incredib1e', 'gains', 'in', 'length', 'of', '3', '4', 'inches', 'to', 'yourpenis', 'permanantly', 'amazing', 'increase', 'in', 'thickness', 'of', 'yourpenis', 'up', 'to', '30', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe'], ['hi', 'peter', 'the', 'hotels', 'are', 'the', 'ones', 'that', 'rent', 'out', 'the', 'tent', 'they', 'are', 'all', 'lined', 'up', 'on', 'the', 'hotel', 'grounds', 'so', 'much', 'for', 'being', 'one', 'with', 'nature', 'more', 'like', 'being', 'one', 'with', 'a', 'couple', 'dozen', 'tour', 'groups', 'and', 'nature', 'i', 'have', 'about', '100m', 'of', 'pictures', 'from', 'that', 'trip', 'i', 'can', 'go', 'through', 'them', 'and', 'get', 'you', 'jpgs', 'of', 'my', 'favorite', 'scenic', 'pictures', 'where', 'are', 'you', 'and', 'jocelyn', 'now', 'new', 'york', 'will', 'you', 'come', 'to', 'tokyo', 'for', 'chinese', 'new', 'year', 'perhaps', 'to', 'see', 'the', 'two', 'of', 'you', 'then', 'i', 'will', 'go', 'to', 'thailand', 'for', 'winter', 'holiday', 'to', 'see', 'my', 'mom', 'take', 'care', 'd'], ['you', 'have', 'everything', 'to', 'gain', 'incredib1e', 'gains', 'in', 'length', 'of', '3', '4', 'inches', 'to', 'yourpenis', 'permanantly', 'amazing', 'increase', 'in', 'thickness', 'of', 'yourpenis', 'up', 'to', '30', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe'], ['yeah', 'i', 'am', 'ready', 'i', 'may', 'not', 'be', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'to', 'germany', 'for', 'me'], ['a', 'home', 'based', 'business', 'opportunity', 'is', 'knocking', 'at', 'your', 'door', 'don', 't', 'be', 'rude', 'and', 'let', 'this', 'chance', 'go', 'by', 'you', 'can', 'earn', 'a', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'to', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts'], ['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'of', 'fractal', 'mathematics', 'and', 'advocate', 'of', 'more', 'sophisticated', 'modelling', 'in', 'quantitative', 'finance', 'died', 'on', '14th', 'october', '2010', 'aged', '85', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'of', 'others', 'inspired', 'by', 'his', 'fundamental', 'insights', 'you', 'must', 'be', 'logged', 'on', 'to', 'view', 'these', 'articles', 'from', 'past', 'issues', 'of', 'wilmott', 'magazine'], ['codeine', 'the', 'most', 'competitive', 'price', 'on', 'net', 'codeine', 'wilson', '30mg', 'x', '30', '156', '00', 'codeine', 'wilson', '30mg', 'x', '60', '291', '00', '4', 'freeviagra', 'pills', 'codeine', 'wilson', '30mg', 'x', '90', '396', '00', '4', 'freeviagra', 'pills', 'codeine', 'wilson', '30mg', 'x', '120', '492', '00', '10', 'freeviagra', 'pills'], ['hi', 'peter', 'sure', 'thing', 'sounds', 'good', 'let', 'me', 'know', 'what', 'time', 'would', 'be', 'good', 'for', 'you', 'i', 'will', 'come', 'prepared', 'with', 'some', 'ideas', 'and', 'we', 'can', 'go', 'from', 'there', 'regards', 'vivek'], ['get', 'up', 'to', '75', 'off', 'at', 'online', 'watchesstore', 'discount', 'watches', 'for', 'all', 'famous', 'brands', 'watches', 'arolexbvlgari', 'dior', 'hermes', 'oris', 'cartier', 'ap', 'and', 'more', 'brands', 'louis', 'vuitton', 'bags', 'wallets', 'gucci', 'bags', 'tiffany', 'co', 'jewerly', 'enjoy', 'a', 'full', '1', 'year', 'warranty', 'shipment', 'via', 'reputable', 'courier', 'fedex', 'ups', 'dhl', 'and', 'ems', 'speedpost', 'you', 'will', '100', 'recieve', 'your', 'order', 'save', 'up', 'to', '75', 'off', 'quality', 'watches'], ['linkedin', 'julius', 'o', 'requested', 'to', 'add', 'you', 'as', 'a', 'connection', 'on', 'linkedin', 'hi', 'peter', 'looking', 'forward', 'to', 'the', 'book', 'accept', 'view', 'invitation', 'from', 'julius', 'o'], ['get', 'up', 'to', '75', 'off', 'at', 'online', 'watchesstore', 'discount', 'watches', 'for', 'all', 'famous', 'brands', 'watches', 'arolexbvlgari', 'dior', 'hermes', 'oris', 'cartier', 'ap', 'and', 'more', 'brands', 'louis', 'vuitton', 'bags', 'wallets', 'gucci', 'bags', 'tiffany', 'co', 'jewerly', 'enjoy', 'a', 'full', '1', 'year', 'warranty', 'shipment', 'via', 'reputable', 'courier', 'fedex', 'ups', 'dhl', 'and', 'ems', 'speedpost', 'you', 'will', '100', 'recieve', 'your', 'order'], ['i', 've', 'thought', 'about', 'this', 'and', 'think', 'it', 's', 'possible', 'we', 'should', 'get', 'another', 'lunch', 'i', 'have', 'a', 'car', 'now', 'and', 'could', 'come', 'pick', 'you', 'up', 'this', 'time', 'does', 'this', 'wednesday', 'work', '11', '50', 'can', 'i', 'have', 'a', 'signed', 'copy', 'of', 'you', 'book'], ['percocet', '10', '625', 'mg', 'withoutprescription', '30', 'tabs', '225', 'percocet', 'a', 'narcotic', 'analgesic', 'is', 'used', 'to', 'treat', 'moderate', 'to', 'moderately', 'severepain', 'top', 'quality', 'express', 'shipping', '100', 'safe', 'discreet', 'private', 'buy', 'cheap', 'percocet', 'online'], ['we', 'saw', 'this', 'on', 'the', 'way', 'to', 'the', 'coast', 'thought', 'u', 'might', 'like', 'it', 'hangzhou', 'is', 'huge', 'one', 'day', 'wasn', 't', 'enough', 'but', 'we', 'got', 'a', 'glimpse', 'we', 'went', 'inside', 'the', 'china', 'pavilion', 'at', 'expo', 'it', 'is', 'pretty', 'interesting', 'each', 'province', 'has', 'an', 'exhibit'], ['get', 'up', 'to', '75', 'off', 'at', 'online', 'watchesstore', 'discount', 'watches', 'for', 'all', 'famous', 'brands', 'watches', 'arolexbvlgari', 'dior', 'hermes', 'oris', 'cartier', 'ap', 'and', 'more', 'brands', 'louis', 'vuitton', 'bags', 'wallets', 'gucci', 'bags', 'tiffany', 'co', 'jewerly', 'enjoy', 'a', 'full', '1', 'year', 'warranty', 'shipment', 'via', 'reputable', 'courier', 'fedex', 'ups', 'dhl', 'and', 'ems', 'speedpost', 'you', 'will', '100', 'recieve', 'your', 'order'], ['hi', 'hommies', 'just', 'got', 'a', 'phone', 'call', 'from', 'the', 'roofer', 'they', 'will', 'come', 'and', 'spaying', 'the', 'foaming', 'today', 'it', 'will', 'be', 'dusty', 'pls', 'close', 'all', 'the', 'doors', 'and', 'windows', 'could', 'you', 'help', 'me', 'to', 'close', 'my', 'bathroom', 'window', 'cat', 'window', 'and', 'the', 'sliding', 'door', 'behind', 'the', 'tv', 'i', 'don', 't', 'know', 'how', 'can', 'those', '2', 'cats', 'survive', 'sorry', 'for', 'any', 'inconvenience'], ['you', 'have', 'everything', 'to', 'gain', 'incredib1e', 'gains', 'in', 'length', 'of', '3', '4', 'inches', 'to', 'yourpenis', 'permanantly', 'amazing', 'increase', 'in', 'thickness', 'of', 'yourpenis', 'up', 'to', '30', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe'], ['scifinance', 'now', 'automatically', 'generates', 'gpu', 'enabled', 'pricing', 'risk', 'model', 'source', 'code', 'that', 'runs', 'up', 'to', '50', '300x', 'faster', 'than', 'serial', 'code', 'using', 'a', 'new', 'nvidia', 'fermi', 'class', 'tesla', '20', 'series', 'gpu', 'scifinance', 'is', 'a', 'derivatives', 'pricing', 'and', 'risk', 'model', 'development', 'tool', 'that', 'automatically', 'generates', 'c', 'c', 'and', 'gpu', 'enabled', 'source', 'code', 'from', 'concise', 'high', 'level', 'model', 'specifications', 'no', 'parallel', 'computing', 'or', 'cuda', 'programming', 'expertise', 'is', 'required', 'scifinance', 's', 'automatic', 'gpu', 'enabled', 'monte', 'carlo', 'pricing', 'model', 'source', 'code', 'generation', 'capabilities', 'have', 'been', 'significantly', 'extended', 'in', 'the', 'latest', 'release', 'this', 'includes'], ['you', 'have', 'everything', 'to', 'gain', 'incredib1e', 'gains', 'in', 'length', 'of', '3', '4', 'inches', 'to', 'yourpenis', 'permanantly', 'amazing', 'increase', 'in', 'thickness', 'of', 'yourpenis', 'up', 'to', '30', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe'], ['ok', 'i', 'will', 'be', 'there', 'by', '10', '00', 'at', 'the', 'latest'], ['experience', 'with', 'biggerpenis', 'today', 'grow', '3', 'inches', 'more', 'the', 'safest', 'most', 'effective', 'methods', 'of_penisen1argement', 'save', 'your', 'time', 'and', 'money', 'bettererections', 'with', 'effective', 'ma1eenhancement', 'products', '1', 'ma1eenhancement', 'supplement', 'trusted', 'by', 'millions', 'buy', 'today'], ['that', 'is', 'cold', 'is', 'there', 'going', 'to', 'be', 'a', 'retirement', 'party', 'are', 'the', 'leaves', 'changing', 'color']]\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# 从文本文件读取数据\n",
    "def loadDataSetFromFile():\n",
    "    listDoc, listClasses = [], []\n",
    "    for i in range(1,26):\n",
    "        wordList = document2WordList(open('../datas/email/spam/%d.txt' % i).read())\n",
    "        listDoc.append(wordList)\n",
    "        listClasses.append(1)\n",
    "        \n",
    "        wordList = document2WordList(open('../datas/email/ham/%d.txt' % i).read())\n",
    "        listDoc.append(wordList)\n",
    "        listClasses.append(0)\n",
    "    return listDoc, listClasses\n",
    "a,b = loadDataSetFromFile()\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49]\n",
      "[8, 47, 22, 46, 20]\n"
     ]
    }
   ],
   "source": [
    "# 训练集、测试集随机划分，取%10\n",
    "def trainTestSplit(totalSize, testSize=0.1, seed=None):\n",
    "    random.seed(seed)\n",
    "    trainingIndexSet = range(totalSize)\n",
    "    testingIndexSet = []\n",
    "    size = int(testSize * totalSize)\n",
    "    for i in range(size):\n",
    "        randIndex = int(random.uniform(0,len(trainingIndexSet)))\n",
    "        testingIndexSet.append(trainingIndexSet[randIndex])\n",
    "        del(trainingIndexSet[randIndex])\n",
    "    return trainingIndexSet, testingIndexSet\n",
    "a,b = trainTestSplit(50)\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest(useSetOf=True, seed=None):\n",
    "    listDoc, listClass = loadDataSetFromFile() # 加载数据\n",
    "    listVocab = createVocabList(listDoc) # 创建词汇表\n",
    "    trainIndexSet, testIndexSet = trainTestSplit(len(listDoc), seed=seed)# 划分数据集\n",
    "    # 训练模型\n",
    "    trainMatrix, trainClasses = [], []\n",
    "    for trainIndex in trainIndexSet:\n",
    "        # 根据useSetOf判断选择词集模型还是词袋模型\n",
    "        trainMatrix.append(setOfWords2Vec(listVocab, listDoc[trainIndex]) if useSetOf else bagOfWords2Vec(listVocab, listDoc[trainIndex]))\n",
    "        trainClasses.append(listClass[trainIndex])\n",
    "    p0Vec, p1Vec, pSpam = trainNB0(trainMatrix, trainClasses)\n",
    "    errorCount = 0\n",
    "    for testIndex in testIndexSet:\n",
    "        testVec = setOfWords2Vec(listVocab, listDoc[testIndex]) if useSetOf else bagOfWords2Vec(listVocab, listDoc[testIndex])\n",
    "        if classifyNB(p0Vec=p0Vec, p1Vec=p1Vec, pClass1=pSpam, vec2Classify=testVec) != listClass[testIndex]:\n",
    "            print 'Error classify: [ %s ]' % listDoc[testIndex]\n",
    "            errorCount += 1\n",
    "    print 'The error rate is: ', 1.0*errorCount/len(listDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-98.8361735559 -112.47798288 -13.6418093245\n",
      "-328.980871199 -281.388376165 47.5924950333\n",
      "-422.121042467 -449.949752574 -27.8287101065\n",
      "-102.081366689 -113.73644387 -11.6550771809\n",
      "-72.0186834379 -78.5113267358 -6.4926432979\n",
      "The error rate is:  0.0\n",
      "-222.644878843 -235.237923783 -12.5930449405\n",
      "Error classify: [ ['a', 'home', 'based', 'business', 'opportunity', 'is', 'knocking', 'at', 'your', 'door', 'don', 't', 'be', 'rude', 'and', 'let', 'this', 'chance', 'go', 'by', 'you', 'can', 'earn', 'a', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'to', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts'] ]\n",
      "-315.323259838 -341.061428051 -25.7381682134\n",
      "-60.6936775141 -65.167132487 -4.47345497286\n",
      "-170.388679222 -187.508609487 -17.1199302654\n",
      "-305.12605964 -328.999369153 -23.8733095123\n",
      "The error rate is:  0.02\n",
      "-231.323109746 -170.80286309 60.5202466558\n",
      "-98.2999033149 -112.175770323 -13.8758670076\n",
      "-170.162892375 -185.758912481 -15.596020106\n",
      "-458.284694693 -484.487789245 -26.2030945523\n",
      "-165.863621539 -141.33559665 24.5280248892\n",
      "The error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    spamTest(seed=i*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-148.387872731 -167.286832861 -18.8989601305\n",
      "-396.988406768 -331.598017648 65.3903891199\n",
      "-580.975855803 -632.359760913 -51.3839051098\n",
      "-149.247673237 -170.748351138 -21.5006779016\n",
      "-80.9460736852 -87.6597155905 -6.71364190523\n",
      "The error rate is:  0.0\n",
      "-254.017233098 -268.533946718 -14.5167136196\n",
      "Error classify: [ ['a', 'home', 'based', 'business', 'opportunity', 'is', 'knocking', 'at', 'your', 'door', 'don', 't', 'be', 'rude', 'and', 'let', 'this', 'chance', 'go', 'by', 'you', 'can', 'earn', 'a', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'to', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts'] ]\n",
      "-448.040520503 -495.797688556 -47.7571680529\n",
      "-62.3685651478 -66.433293381 -4.06472823324\n",
      "-193.498425777 -215.427956274 -21.9295304971\n",
      "-357.569761316 -386.257740129 -28.6879788124\n",
      "The error rate is:  0.02\n",
      "-283.963072141 -205.593907394 78.3691647472\n",
      "-147.227195366 -166.944762288 -19.7175669225\n",
      "-191.665625614 -213.423763299 -21.7581376853\n",
      "-639.982968132 -680.956302758 -40.9733346256\n",
      "-208.774662274 -170.664863574 38.1097986998\n",
      "The error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    spamTest(useSetOf=False, seed=i*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果看，词袋模型没有能够得到比词集模型更好的结果，但是从p0和p1的差值来看，词袋模型的结果差值更大，这保证了在某些很接近的case下，可能词集模型无法正确判断，而词袋模型依然具有良好的鲁棒性，类似SVM中，词袋模型就是具有更好的distance的那一种；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例：使用NB从个人广告中获取区域倾向\n",
    "\n",
    "NB在分类以外的应用，解释NB分类器训练得到的知识；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "对于分类任务，有时输出概率要优于输出一个确定的类别，贝叶斯概率以及准则提供了一种利用已知值来估计为止概率的方法，通过假设特征之间独立，可以降低对数据量的要求，因此称之为NB，NB的两种实现：贝努利模型、多项式模型，贝努利模型只关注词是否出现，而不关注出现次数，即一个对应词集模型，一个对应词袋模型，对比来说，词袋模型在分类时，要更加明确（即类别之间的差异更大，鲁棒性更好）；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
